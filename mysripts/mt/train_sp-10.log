2024-01-07 11:47:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 256, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 1000, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 256, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10', 'restore_file': '/home2/chu/fairseq_sign/sign/pretrained_models/mbart.cc25.v2/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='mbart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='mbart_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index', data_buffer_size=10, dataset_impl=None, ddp_backend='legacy_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=5, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.2, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_source_positions=1024, max_target_positions=1024, max_tokens=256, max_tokens_valid=256, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home2/chu/fairseq_sign/sign/pretrained_models/mbart.cc25.v2/model.pt', save_dir='/home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10', save_interval=1, save_interval_updates=0, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='de_DE', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=1000, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='translation_from_pretrained_bart', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='mbart_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index', data_buffer_size=10, dataset_impl=None, ddp_backend='legacy_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=5, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.2, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_source_positions=1024, max_target_positions=1024, max_tokens=256, max_tokens_valid=256, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home2/chu/fairseq_sign/sign/pretrained_models/mbart.cc25.v2/model.pt', save_dir='/home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10', save_interval=1, save_interval_updates=0, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='de_DE', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=1000, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 2000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-01-07 11:47:38 | INFO | fairseq.tasks.translation | [de_DE] dictionary: 250001 types
2024-01-07 11:47:38 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 250001 types
2024-01-07 11:47:54 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250027, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250027, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=250027, bias=False)
  )
  (classification_heads): ModuleDict()
)
2024-01-07 11:47:54 | INFO | fairseq_cli.train | task: TranslationFromPretrainedBARTTask
2024-01-07 11:47:54 | INFO | fairseq_cli.train | model: BARTModel
2024-01-07 11:47:54 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-01-07 11:47:54 | INFO | fairseq_cli.train | num. shared model params: 610,851,840 (num. trained: 610,851,840)
2024-01-07 11:47:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-01-07 11:47:54 | INFO | fairseq.data.data_utils | loaded 142 examples from: /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index/valid.de_DE-en_XX.de_DE
2024-01-07 11:47:54 | INFO | fairseq.data.data_utils | loaded 142 examples from: /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index/valid.de_DE-en_XX.en_XX
2024-01-07 11:47:54 | INFO | fairseq.tasks.translation | /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index valid de_DE-en_XX 142 examples
2024-01-07 11:47:56 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-01-07 11:47:56 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-01-07 11:47:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-07 11:47:56 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2024-01-07 11:47:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-01-07 11:47:56 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-01-07 11:47:56 | INFO | fairseq_cli.train | max tokens per device = 256 and max sentences per device = None
2024-01-07 11:47:56 | INFO | fairseq.trainer | Preparing to load checkpoint /home2/chu/fairseq_sign/sign/pretrained_models/mbart.cc25.v2/model.pt
2024-01-07 11:48:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-01-07 11:48:09 | INFO | fairseq.trainer | Loaded checkpoint /home2/chu/fairseq_sign/sign/pretrained_models/mbart.cc25.v2/model.pt (epoch 142 @ 0 updates)
2024-01-07 11:48:10 | INFO | fairseq.trainer | loading train data for epoch 1
2024-01-07 11:48:10 | INFO | fairseq.data.data_utils | loaded 830 examples from: /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index/train.de_DE-en_XX.de_DE
2024-01-07 11:48:10 | INFO | fairseq.data.data_utils | loaded 830 examples from: /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index/train.de_DE-en_XX.en_XX
2024-01-07 11:48:10 | INFO | fairseq.tasks.translation | /home2/chu/fairseq_sign/sign/dataset/sp-10/index/de-en/mbart_index train de_DE-en_XX 830 examples
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-07 11:48:10 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-01-07 11:48:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-01-07 11:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:48:10 | INFO | fairseq.trainer | begin training epoch 1
2024-01-07 11:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 40 updates
2024-01-07 11:48:17 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 1 @ 40 updates, score None) (writing took 24.931786171975546 seconds)
2024-01-07 11:48:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-01-07 11:48:42 | INFO | train | epoch 001 | loss 35.245 | nll_loss 13.977 | ppl 16122.1 | wps 223.7 | ups 1.25 | wpb 176.4 | bsz 20.6 | num_updates 40 | lr 2.4e-06 | gnorm 519.679 | train_wall 7 | gb_free 12.7 | wall 47
2024-01-07 11:48:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:48:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:48:42 | INFO | fairseq.trainer | begin training epoch 2
2024-01-07 11:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:48:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 80 updates
2024-01-07 11:48:49 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 2 @ 80 updates, score None) (writing took 39.72427169099683 seconds)
2024-01-07 11:49:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-01-07 11:49:28 | INFO | train | epoch 002 | loss 17.837 | nll_loss 12.452 | ppl 5602.03 | wps 153.2 | ups 0.87 | wpb 176.4 | bsz 20.6 | num_updates 80 | lr 4.8e-06 | gnorm 92.547 | train_wall 6 | gb_free 12.7 | wall 93
2024-01-07 11:49:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:49:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:49:28 | INFO | fairseq.trainer | begin training epoch 3
2024-01-07 11:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:49:32 | INFO | train_inner | epoch 003:     20 / 40 loss=24.356, nll_loss=12.598, ppl=6199.5, wps=218.6, ups=1.23, wpb=176.4, bsz=20.7, num_updates=100, lr=6e-06, gnorm=261.602, train_wall=16, gb_free=12.7, wall=96
2024-01-07 11:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 120 updates
2024-01-07 11:49:35 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:50:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 3 @ 120 updates, score None) (writing took 34.17250695696566 seconds)
2024-01-07 11:50:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-01-07 11:50:09 | INFO | train | epoch 003 | loss 15.288 | nll_loss 9.844 | ppl 919.22 | wps 175 | ups 0.99 | wpb 176.4 | bsz 20.6 | num_updates 120 | lr 7.2e-06 | gnorm 116.311 | train_wall 6 | gb_free 12.7 | wall 133
2024-01-07 11:50:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:50:09 | INFO | fairseq.trainer | begin training epoch 4
2024-01-07 11:50:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 160 updates
2024-01-07 11:50:15 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:50:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 4 @ 160 updates, score None) (writing took 27.314806646027137 seconds)
2024-01-07 11:50:42 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-01-07 11:50:42 | INFO | train | epoch 004 | loss 13.749 | nll_loss 8.442 | ppl 347.67 | wps 210.8 | ups 1.2 | wpb 176.4 | bsz 20.6 | num_updates 160 | lr 9.6e-06 | gnorm 59.228 | train_wall 6 | gb_free 12.7 | wall 167
2024-01-07 11:50:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:50:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:50:42 | INFO | fairseq.trainer | begin training epoch 5
2024-01-07 11:50:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:50:48 | INFO | train_inner | epoch 005:     40 / 40 loss=13.541, nll_loss=8.243, ppl=302.99, wps=229.7, ups=1.3, wpb=176.3, bsz=20.5, num_updates=200, lr=1.2e-05, gnorm=67.396, train_wall=15, gb_free=12.7, wall=173
2024-01-07 11:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 200 updates
2024-01-07 11:50:48 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 5 @ 200 updates, score None) (writing took 29.203033461992163 seconds)
2024-01-07 11:51:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-01-07 11:51:17 | INFO | train | epoch 005 | loss 12.631 | nll_loss 7.392 | ppl 167.93 | wps 199.8 | ups 1.13 | wpb 176.4 | bsz 20.6 | num_updates 200 | lr 1.2e-05 | gnorm 34.73 | train_wall 6 | gb_free 12.7 | wall 202
2024-01-07 11:51:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:51:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:51:17 | INFO | fairseq.trainer | begin training epoch 6
2024-01-07 11:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:51:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 240 updates
2024-01-07 11:51:24 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:51:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 6 @ 240 updates, score None) (writing took 33.658826956991106 seconds)
2024-01-07 11:51:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-01-07 11:51:57 | INFO | train | epoch 006 | loss 11.493 | nll_loss 6.289 | ppl 78.2 | wps 177.4 | ups 1.01 | wpb 176.4 | bsz 20.6 | num_updates 240 | lr 1.44e-05 | gnorm 33.628 | train_wall 6 | gb_free 12.7 | wall 242
2024-01-07 11:51:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:51:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:51:57 | INFO | fairseq.trainer | begin training epoch 7
2024-01-07 11:51:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 280 updates
2024-01-07 11:52:03 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:52:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:52:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 7 @ 280 updates, score None) (writing took 36.646634722012095 seconds)
2024-01-07 11:52:40 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-01-07 11:52:40 | INFO | train | epoch 007 | loss 10.615 | nll_loss 5.433 | ppl 43.22 | wps 164.9 | ups 0.93 | wpb 176.4 | bsz 20.6 | num_updates 280 | lr 1.68e-05 | gnorm 32.078 | train_wall 6 | gb_free 12.7 | wall 284
2024-01-07 11:52:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:52:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:52:40 | INFO | fairseq.trainer | begin training epoch 8
2024-01-07 11:52:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:52:43 | INFO | train_inner | epoch 008:     20 / 40 loss=10.833, nll_loss=5.649, ppl=50.16, wps=152.1, ups=0.87, wpb=174.7, bsz=20.4, num_updates=300, lr=1.8e-05, gnorm=30.572, train_wall=15, gb_free=12.7, wall=287
2024-01-07 11:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 320 updates
2024-01-07 11:52:46 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:53:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 8 @ 320 updates, score None) (writing took 35.299247648974415 seconds)
2024-01-07 11:53:21 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-01-07 11:53:22 | INFO | train | epoch 008 | loss 9.749 | nll_loss 4.596 | ppl 24.18 | wps 170.2 | ups 0.96 | wpb 176.4 | bsz 20.6 | num_updates 320 | lr 1.92e-05 | gnorm 20.282 | train_wall 6 | gb_free 12.7 | wall 326
2024-01-07 11:53:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:53:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:53:22 | INFO | fairseq.trainer | begin training epoch 9
2024-01-07 11:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:53:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 360 updates
2024-01-07 11:53:28 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 9 @ 360 updates, score None) (writing took 36.8472861390328 seconds)
2024-01-07 11:54:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-01-07 11:54:05 | INFO | train | epoch 009 | loss 9.306 | nll_loss 4.187 | ppl 18.22 | wps 164 | ups 0.93 | wpb 176.4 | bsz 20.6 | num_updates 360 | lr 2.16e-05 | gnorm 33.311 | train_wall 6 | gb_free 12.7 | wall 369
2024-01-07 11:54:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:54:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:54:05 | INFO | fairseq.trainer | begin training epoch 10
2024-01-07 11:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:54:11 | INFO | train_inner | epoch 010:     40 / 40 loss=9.249, nll_loss=4.126, ppl=17.46, wps=203.3, ups=1.14, wpb=178, bsz=20.8, num_updates=400, lr=2.4e-05, gnorm=28.574, train_wall=15, gb_free=12.7, wall=375
2024-01-07 11:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 400 updates
2024-01-07 11:54:11 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:54:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 10 @ 400 updates, score None) (writing took 33.03225608996581 seconds)
2024-01-07 11:54:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-01-07 11:54:44 | INFO | train | epoch 010 | loss 9.007 | nll_loss 3.895 | ppl 14.88 | wps 179.8 | ups 1.02 | wpb 176.4 | bsz 20.6 | num_updates 400 | lr 2.4e-05 | gnorm 28.568 | train_wall 6 | gb_free 12.7 | wall 408
2024-01-07 11:54:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:54:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:54:44 | INFO | fairseq.trainer | begin training epoch 11
2024-01-07 11:54:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:54:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 440 updates
2024-01-07 11:54:50 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 11 @ 440 updates, score None) (writing took 28.27379904902773 seconds)
2024-01-07 11:55:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-01-07 11:55:18 | INFO | train | epoch 011 | loss 8.562 | nll_loss 3.526 | ppl 11.52 | wps 204.3 | ups 1.16 | wpb 176.4 | bsz 20.6 | num_updates 440 | lr 2.64e-05 | gnorm 16.898 | train_wall 6 | gb_free 12.7 | wall 443
2024-01-07 11:55:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:55:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:55:18 | INFO | fairseq.trainer | begin training epoch 12
2024-01-07 11:55:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 480 updates
2024-01-07 11:55:24 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:55:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:55:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 12 @ 480 updates, score None) (writing took 29.034975504968315 seconds)
2024-01-07 11:55:53 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-01-07 11:55:53 | INFO | train | epoch 012 | loss 8.49 | nll_loss 3.537 | ppl 11.61 | wps 200.5 | ups 1.14 | wpb 176.4 | bsz 20.6 | num_updates 480 | lr 2.88e-05 | gnorm 17.363 | train_wall 6 | gb_free 12.7 | wall 478
2024-01-07 11:55:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:55:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:55:53 | INFO | fairseq.trainer | begin training epoch 13
2024-01-07 11:55:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:55:57 | INFO | train_inner | epoch 013:     20 / 40 loss=8.824, nll_loss=3.843, ppl=14.35, wps=165.8, ups=0.94, wpb=175.5, bsz=20.7, num_updates=500, lr=3e-05, gnorm=22.015, train_wall=15, gb_free=12.7, wall=481
2024-01-07 11:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 520 updates
2024-01-07 11:56:00 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 13 @ 520 updates, score None) (writing took 29.33974694396602 seconds)
2024-01-07 11:56:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-01-07 11:56:29 | INFO | train | epoch 013 | loss 9.211 | nll_loss 4.337 | ppl 20.21 | wps 198.9 | ups 1.13 | wpb 176.4 | bsz 20.6 | num_updates 520 | lr 2.96e-05 | gnorm 32.526 | train_wall 6 | gb_free 12.7 | wall 513
2024-01-07 11:56:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:56:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:56:29 | INFO | fairseq.trainer | begin training epoch 14
2024-01-07 11:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 560 updates
2024-01-07 11:56:35 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:57:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:57:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 14 @ 560 updates, score None) (writing took 34.67568314902019 seconds)
2024-01-07 11:57:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-01-07 11:57:10 | INFO | train | epoch 014 | loss 7.229 | nll_loss 2.467 | ppl 5.53 | wps 172.9 | ups 0.98 | wpb 176.4 | bsz 20.6 | num_updates 560 | lr 2.88e-05 | gnorm 13.567 | train_wall 6 | gb_free 12.7 | wall 554
2024-01-07 11:57:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:57:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:57:10 | INFO | fairseq.trainer | begin training epoch 15
2024-01-07 11:57:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:57:16 | INFO | train_inner | epoch 015:     40 / 40 loss=7.292, nll_loss=2.557, ppl=5.89, wps=223.4, ups=1.26, wpb=177.2, bsz=20.5, num_updates=600, lr=2.8e-05, gnorm=14.105, train_wall=15, gb_free=12.7, wall=560
2024-01-07 11:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 600 updates
2024-01-07 11:57:16 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:57:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 15 @ 600 updates, score None) (writing took 32.57135060598375 seconds)
2024-01-07 11:57:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-01-07 11:57:48 | INFO | train | epoch 015 | loss 6.78 | nll_loss 2.117 | ppl 4.34 | wps 182.2 | ups 1.03 | wpb 176.4 | bsz 20.6 | num_updates 600 | lr 2.8e-05 | gnorm 9.946 | train_wall 6 | gb_free 12.7 | wall 593
2024-01-07 11:57:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:57:48 | INFO | fairseq.trainer | begin training epoch 16
2024-01-07 11:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 640 updates
2024-01-07 11:57:55 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:58:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 16 @ 640 updates, score None) (writing took 35.95547277899459 seconds)
2024-01-07 11:58:31 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-01-07 11:58:31 | INFO | train | epoch 016 | loss 6.599 | nll_loss 1.994 | ppl 3.98 | wps 167.5 | ups 0.95 | wpb 176.4 | bsz 20.6 | num_updates 640 | lr 2.72e-05 | gnorm 11.508 | train_wall 6 | gb_free 12.7 | wall 635
2024-01-07 11:58:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:58:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:58:31 | INFO | fairseq.trainer | begin training epoch 17
2024-01-07 11:58:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 680 updates
2024-01-07 11:58:37 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 17 @ 680 updates, score None) (writing took 31.615959224000107 seconds)
2024-01-07 11:59:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-01-07 11:59:08 | INFO | train | epoch 017 | loss 6.422 | nll_loss 1.86 | ppl 3.63 | wps 186.8 | ups 1.06 | wpb 176.4 | bsz 20.6 | num_updates 680 | lr 2.64e-05 | gnorm 11.796 | train_wall 6 | gb_free 12.7 | wall 673
2024-01-07 11:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:59:08 | INFO | fairseq.trainer | begin training epoch 18
2024-01-07 11:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:59:11 | INFO | train_inner | epoch 018:     20 / 40 loss=6.463, nll_loss=1.887, ppl=3.7, wps=151.3, ups=0.87, wpb=174.8, bsz=20.4, num_updates=700, lr=2.6e-05, gnorm=11.975, train_wall=15, gb_free=12.7, wall=676
2024-01-07 11:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 720 updates
2024-01-07 11:59:15 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:59:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 11:59:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 18 @ 720 updates, score None) (writing took 27.945257290964946 seconds)
2024-01-07 11:59:43 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-01-07 11:59:43 | INFO | train | epoch 018 | loss 6.269 | nll_loss 1.735 | ppl 3.33 | wps 206.2 | ups 1.17 | wpb 176.4 | bsz 20.6 | num_updates 720 | lr 2.56e-05 | gnorm 13.535 | train_wall 6 | gb_free 12.7 | wall 707
2024-01-07 11:59:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 11:59:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 11:59:43 | INFO | fairseq.trainer | begin training epoch 19
2024-01-07 11:59:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 11:59:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 760 updates
2024-01-07 11:59:49 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:00:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 19 @ 760 updates, score None) (writing took 28.695372764021158 seconds)
2024-01-07 12:00:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-01-07 12:00:18 | INFO | train | epoch 019 | loss 6.128 | nll_loss 1.609 | ppl 3.05 | wps 201.9 | ups 1.14 | wpb 176.4 | bsz 20.6 | num_updates 760 | lr 2.48e-05 | gnorm 9.57 | train_wall 6 | gb_free 12.7 | wall 742
2024-01-07 12:00:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:00:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:00:18 | INFO | fairseq.trainer | begin training epoch 20
2024-01-07 12:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:00:24 | INFO | train_inner | epoch 020:     40 / 40 loss=6.106, nll_loss=1.602, ppl=3.04, wps=246.4, ups=1.38, wpb=177.9, bsz=20.8, num_updates=800, lr=2.4e-05, gnorm=9.676, train_wall=15, gb_free=12.7, wall=748
2024-01-07 12:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 800 updates
2024-01-07 12:00:24 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:00:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:00:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 20 @ 800 updates, score None) (writing took 33.9454887949978 seconds)
2024-01-07 12:00:58 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-01-07 12:00:58 | INFO | train | epoch 020 | loss 5.994 | nll_loss 1.518 | ppl 2.86 | wps 176 | ups 1 | wpb 176.4 | bsz 20.6 | num_updates 800 | lr 2.4e-05 | gnorm 7.718 | train_wall 6 | gb_free 12.7 | wall 782
2024-01-07 12:00:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:00:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:00:58 | INFO | fairseq.trainer | begin training epoch 21
2024-01-07 12:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 840 updates
2024-01-07 12:01:04 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 21 @ 840 updates, score None) (writing took 32.14467494399287 seconds)
2024-01-07 12:01:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-01-07 12:01:36 | INFO | train | epoch 021 | loss 5.895 | nll_loss 1.444 | ppl 2.72 | wps 183.8 | ups 1.04 | wpb 176.4 | bsz 20.6 | num_updates 840 | lr 2.32e-05 | gnorm 7.982 | train_wall 6 | gb_free 12.7 | wall 820
2024-01-07 12:01:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:01:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:01:36 | INFO | fairseq.trainer | begin training epoch 22
2024-01-07 12:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 880 updates
2024-01-07 12:01:42 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:02:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 22 @ 880 updates, score None) (writing took 28.937792620970868 seconds)
2024-01-07 12:02:11 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-01-07 12:02:11 | INFO | train | epoch 022 | loss 5.77 | nll_loss 1.354 | ppl 2.56 | wps 200.9 | ups 1.14 | wpb 176.4 | bsz 20.6 | num_updates 880 | lr 2.24e-05 | gnorm 8.263 | train_wall 6 | gb_free 12.7 | wall 855
2024-01-07 12:02:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:02:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:02:11 | INFO | fairseq.trainer | begin training epoch 23
2024-01-07 12:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:02:14 | INFO | train_inner | epoch 023:     20 / 40 loss=5.815, nll_loss=1.39, ppl=2.62, wps=157.6, ups=0.9, wpb=174.2, bsz=20.4, num_updates=900, lr=2.2e-05, gnorm=13.052, train_wall=15, gb_free=12.7, wall=859
2024-01-07 12:02:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 920 updates
2024-01-07 12:02:17 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 23 @ 920 updates, score None) (writing took 28.509853766008746 seconds)
2024-01-07 12:02:46 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-01-07 12:02:46 | INFO | train | epoch 023 | loss 5.712 | nll_loss 1.322 | ppl 2.5 | wps 203.7 | ups 1.15 | wpb 176.4 | bsz 20.6 | num_updates 920 | lr 2.16e-05 | gnorm 20.49 | train_wall 6 | gb_free 12.7 | wall 890
2024-01-07 12:02:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:02:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:02:46 | INFO | fairseq.trainer | begin training epoch 24
2024-01-07 12:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:02:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 960 updates
2024-01-07 12:02:52 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 24 @ 960 updates, score None) (writing took 30.705496958980802 seconds)
2024-01-07 12:03:23 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-01-07 12:03:23 | INFO | train | epoch 024 | loss 5.612 | nll_loss 1.242 | ppl 2.36 | wps 191.6 | ups 1.09 | wpb 176.4 | bsz 20.6 | num_updates 960 | lr 2.08e-05 | gnorm 8.864 | train_wall 6 | gb_free 12.7 | wall 927
2024-01-07 12:03:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:03:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:03:23 | INFO | fairseq.trainer | begin training epoch 25
2024-01-07 12:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:03:29 | INFO | train_inner | epoch 025:     40 / 40 loss=5.606, nll_loss=1.242, ppl=2.37, wps=239.4, ups=1.34, wpb=178.6, bsz=20.8, num_updates=1000, lr=2e-05, gnorm=8.188, train_wall=15, gb_free=12.7, wall=933
2024-01-07 12:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:03:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:03:29 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.273 | nll_loss 2.268 | ppl 4.82 | wps 4721.2 | wpb 143.6 | bsz 17 | num_updates 1000
2024-01-07 12:03:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1000 updates
2024-01-07 12:03:29 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:03:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 25 @ 1000 updates, score 6.273) (writing took 125.40497376502026 seconds)
2024-01-07 12:05:35 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-01-07 12:05:35 | INFO | train | epoch 025 | loss 5.557 | nll_loss 1.214 | ppl 2.32 | wps 53.5 | ups 0.3 | wpb 176.4 | bsz 20.6 | num_updates 1000 | lr 2e-05 | gnorm 7.499 | train_wall 6 | gb_free 12.7 | wall 1059
2024-01-07 12:05:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:05:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:05:35 | INFO | fairseq.trainer | begin training epoch 26
2024-01-07 12:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:05:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:05:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:05:41 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.271 | nll_loss 2.248 | ppl 4.75 | wps 4584.5 | wpb 143.6 | bsz 17 | num_updates 1040 | best_loss 6.271
2024-01-07 12:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1040 updates
2024-01-07 12:05:41 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:06:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:08:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 26 @ 1040 updates, score 6.271) (writing took 138.88807411299786 seconds)
2024-01-07 12:08:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-01-07 12:08:00 | INFO | train | epoch 026 | loss 5.485 | nll_loss 1.168 | ppl 2.25 | wps 48.5 | ups 0.28 | wpb 176.4 | bsz 20.6 | num_updates 1040 | lr 1.92e-05 | gnorm 6.61 | train_wall 6 | gb_free 12.7 | wall 1204
2024-01-07 12:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:08:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:08:00 | INFO | fairseq.trainer | begin training epoch 27
2024-01-07 12:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:08:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:08:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.254 | nll_loss 2.284 | ppl 4.87 | wps 4295.9 | wpb 143.6 | bsz 17 | num_updates 1080 | best_loss 6.254
2024-01-07 12:08:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1080 updates
2024-01-07 12:08:06 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:08:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 27 @ 1080 updates, score 6.254) (writing took 112.14241698896512 seconds)
2024-01-07 12:09:59 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-01-07 12:09:59 | INFO | train | epoch 027 | loss 5.44 | nll_loss 1.118 | ppl 2.17 | wps 59.4 | ups 0.34 | wpb 176.4 | bsz 20.6 | num_updates 1080 | lr 1.84e-05 | gnorm 11.783 | train_wall 6 | gb_free 12.7 | wall 1323
2024-01-07 12:09:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:09:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:09:59 | INFO | fairseq.trainer | begin training epoch 28
2024-01-07 12:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:10:02 | INFO | train_inner | epoch 028:     20 / 40 loss=5.459, nll_loss=1.142, ppl=2.21, wps=45.8, ups=0.25, wpb=179.9, bsz=21.4, num_updates=1100, lr=1.8e-05, gnorm=8.557, train_wall=15, gb_free=12.7, wall=1326
2024-01-07 12:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:10:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:10:05 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.262 | nll_loss 2.371 | ppl 5.17 | wps 4607.2 | wpb 143.6 | bsz 17 | num_updates 1120 | best_loss 6.254
2024-01-07 12:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1120 updates
2024-01-07 12:10:05 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2620.pt
2024-01-07 12:10:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2620.pt
2024-01-07 12:11:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2620.pt (epoch 28 @ 1120 updates, score 6.262) (writing took 83.46697123500053 seconds)
2024-01-07 12:11:29 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-01-07 12:11:29 | INFO | train | epoch 028 | loss 5.399 | nll_loss 1.086 | ppl 2.12 | wps 78.3 | ups 0.44 | wpb 176.4 | bsz 20.6 | num_updates 1120 | lr 1.76e-05 | gnorm 6.691 | train_wall 6 | gb_free 12.7 | wall 1413
2024-01-07 12:11:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:11:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:11:29 | INFO | fairseq.trainer | begin training epoch 29
2024-01-07 12:11:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:11:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:11:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:11:35 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.252 | nll_loss 2.373 | ppl 5.18 | wps 4592.8 | wpb 143.6 | bsz 17 | num_updates 1160 | best_loss 6.252
2024-01-07 12:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1160 updates
2024-01-07 12:11:35 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:12:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:13:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 29 @ 1160 updates, score 6.252) (writing took 130.37880078097805 seconds)
2024-01-07 12:13:46 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-01-07 12:13:46 | INFO | train | epoch 029 | loss 5.355 | nll_loss 1.064 | ppl 2.09 | wps 51.5 | ups 0.29 | wpb 176.4 | bsz 20.6 | num_updates 1160 | lr 1.68e-05 | gnorm 6.301 | train_wall 6 | gb_free 12.7 | wall 1550
2024-01-07 12:13:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:13:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:13:46 | INFO | fairseq.trainer | begin training epoch 30
2024-01-07 12:13:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:13:52 | INFO | train_inner | epoch 030:     40 / 40 loss=5.338, nll_loss=1.042, ppl=2.06, wps=75.2, ups=0.43, wpb=172.9, bsz=19.8, num_updates=1200, lr=1.6e-05, gnorm=6.198, train_wall=15, gb_free=12.7, wall=1556
2024-01-07 12:13:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:13:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:13:52 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.224 | nll_loss 2.354 | ppl 5.11 | wps 4651.5 | wpb 143.6 | bsz 17 | num_updates 1200 | best_loss 6.224
2024-01-07 12:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1200 updates
2024-01-07 12:13:52 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:14:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:15:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 30 @ 1200 updates, score 6.224) (writing took 93.46495721302927 seconds)
2024-01-07 12:15:26 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-01-07 12:15:26 | INFO | train | epoch 030 | loss 5.318 | nll_loss 1.03 | ppl 2.04 | wps 70.3 | ups 0.4 | wpb 176.4 | bsz 20.6 | num_updates 1200 | lr 1.6e-05 | gnorm 5.502 | train_wall 6 | gb_free 12.7 | wall 1650
2024-01-07 12:15:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:15:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:15:26 | INFO | fairseq.trainer | begin training epoch 31
2024-01-07 12:15:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:15:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:15:32 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.24 | nll_loss 2.4 | ppl 5.28 | wps 4690.9 | wpb 143.6 | bsz 17 | num_updates 1240 | best_loss 6.224
2024-01-07 12:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1240 updates
2024-01-07 12:15:32 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2403.pt
2024-01-07 12:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2403.pt
2024-01-07 12:16:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2403.pt (epoch 31 @ 1240 updates, score 6.24) (writing took 51.019080411992036 seconds)
2024-01-07 12:16:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-01-07 12:16:24 | INFO | train | epoch 031 | loss 5.279 | nll_loss 1.001 | ppl 2 | wps 122.5 | ups 0.69 | wpb 176.4 | bsz 20.6 | num_updates 1240 | lr 1.52e-05 | gnorm 5.442 | train_wall 6 | gb_free 12.7 | wall 1708
2024-01-07 12:16:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:16:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:16:24 | INFO | fairseq.trainer | begin training epoch 32
2024-01-07 12:16:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:16:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:16:30 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.231 | nll_loss 2.38 | ppl 5.21 | wps 4670.9 | wpb 143.6 | bsz 17 | num_updates 1280 | best_loss 6.224
2024-01-07 12:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1280 updates
2024-01-07 12:16:30 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2313.pt
2024-01-07 12:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2313.pt
2024-01-07 12:17:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.2313.pt (epoch 32 @ 1280 updates, score 6.231) (writing took 54.99754511396168 seconds)
2024-01-07 12:17:25 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-01-07 12:17:25 | INFO | train | epoch 032 | loss 5.243 | nll_loss 0.964 | ppl 1.95 | wps 114.5 | ups 0.65 | wpb 176.4 | bsz 20.6 | num_updates 1280 | lr 1.44e-05 | gnorm 5.386 | train_wall 6 | gb_free 12.7 | wall 1770
2024-01-07 12:17:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:17:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:17:25 | INFO | fairseq.trainer | begin training epoch 33
2024-01-07 12:17:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:17:28 | INFO | train_inner | epoch 033:     20 / 40 loss=5.255, nll_loss=0.98, ppl=1.97, wps=81.4, ups=0.46, wpb=176.3, bsz=20.5, num_updates=1300, lr=1.4e-05, gnorm=5.477, train_wall=15, gb_free=12.7, wall=1773
2024-01-07 12:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:17:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:17:32 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.205 | nll_loss 2.331 | ppl 5.03 | wps 4710.5 | wpb 143.6 | bsz 17 | num_updates 1320 | best_loss 6.205
2024-01-07 12:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1320 updates
2024-01-07 12:17:32 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 33 @ 1320 updates, score 6.205) (writing took 97.57865493703866 seconds)
2024-01-07 12:19:09 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-01-07 12:19:10 | INFO | train | epoch 033 | loss 5.216 | nll_loss 0.951 | ppl 1.93 | wps 67.7 | ups 0.38 | wpb 176.4 | bsz 20.6 | num_updates 1320 | lr 1.36e-05 | gnorm 5.641 | train_wall 6 | gb_free 12.7 | wall 1874
2024-01-07 12:19:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:19:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:19:10 | INFO | fairseq.trainer | begin training epoch 34
2024-01-07 12:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:19:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:19:16 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.204 | nll_loss 2.359 | ppl 5.13 | wps 4664.5 | wpb 143.6 | bsz 17 | num_updates 1360 | best_loss 6.204
2024-01-07 12:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1360 updates
2024-01-07 12:19:16 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 34 @ 1360 updates, score 6.204) (writing took 99.75701453298097 seconds)
2024-01-07 12:20:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-01-07 12:20:56 | INFO | train | epoch 034 | loss 5.191 | nll_loss 0.927 | ppl 1.9 | wps 66.4 | ups 0.38 | wpb 176.4 | bsz 20.6 | num_updates 1360 | lr 1.28e-05 | gnorm 6.021 | train_wall 6 | gb_free 12.7 | wall 1980
2024-01-07 12:20:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:20:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:20:56 | INFO | fairseq.trainer | begin training epoch 35
2024-01-07 12:20:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:21:02 | INFO | train_inner | epoch 035:     40 / 40 loss=5.182, nll_loss=0.92, ppl=1.89, wps=82.6, ups=0.47, wpb=176.5, bsz=20.7, num_updates=1400, lr=1.2e-05, gnorm=5.692, train_wall=15, gb_free=12.7, wall=1986
2024-01-07 12:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:21:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:21:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.184 | nll_loss 2.309 | ppl 4.96 | wps 4728.8 | wpb 143.6 | bsz 17 | num_updates 1400 | best_loss 6.184
2024-01-07 12:21:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1400 updates
2024-01-07 12:21:02 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:22:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 35 @ 1400 updates, score 6.184) (writing took 92.77616707701236 seconds)
2024-01-07 12:22:35 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-01-07 12:22:35 | INFO | train | epoch 035 | loss 5.162 | nll_loss 0.906 | ppl 1.87 | wps 71 | ups 0.4 | wpb 176.4 | bsz 20.6 | num_updates 1400 | lr 1.2e-05 | gnorm 5.433 | train_wall 6 | gb_free 12.7 | wall 2080
2024-01-07 12:22:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:22:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:22:35 | INFO | fairseq.trainer | begin training epoch 36
2024-01-07 12:22:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:22:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:22:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:22:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.172 | nll_loss 2.319 | ppl 4.99 | wps 4673 | wpb 143.6 | bsz 17 | num_updates 1440 | best_loss 6.172
2024-01-07 12:22:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1440 updates
2024-01-07 12:22:42 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:24:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 36 @ 1440 updates, score 6.172) (writing took 113.53249136795057 seconds)
2024-01-07 12:24:36 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-01-07 12:24:36 | INFO | train | epoch 036 | loss 5.15 | nll_loss 0.895 | ppl 1.86 | wps 58.5 | ups 0.33 | wpb 176.4 | bsz 20.6 | num_updates 1440 | lr 1.12e-05 | gnorm 5.229 | train_wall 7 | gb_free 12.7 | wall 2200
2024-01-07 12:24:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:24:36 | INFO | fairseq.trainer | begin training epoch 37
2024-01-07 12:24:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:24:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:24:42 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.196 | nll_loss 2.407 | ppl 5.31 | wps 4735.1 | wpb 143.6 | bsz 17 | num_updates 1480 | best_loss 6.172
2024-01-07 12:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1480 updates
2024-01-07 12:24:42 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1964.pt
2024-01-07 12:25:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1964.pt
2024-01-07 12:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1964.pt (epoch 37 @ 1480 updates, score 6.196) (writing took 55.717554948991165 seconds)
2024-01-07 12:25:38 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-01-07 12:25:38 | INFO | train | epoch 037 | loss 5.125 | nll_loss 0.875 | ppl 1.83 | wps 113.4 | ups 0.64 | wpb 176.4 | bsz 20.6 | num_updates 1480 | lr 1.04e-05 | gnorm 5.046 | train_wall 6 | gb_free 12.7 | wall 2262
2024-01-07 12:25:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:25:38 | INFO | fairseq.trainer | begin training epoch 38
2024-01-07 12:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:25:41 | INFO | train_inner | epoch 038:     20 / 40 loss=5.134, nll_loss=0.884, ppl=1.85, wps=62.9, ups=0.36, wpb=175.5, bsz=20.6, num_updates=1500, lr=1e-05, gnorm=5.153, train_wall=15, gb_free=12.7, wall=2266
2024-01-07 12:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:25:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:25:45 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.181 | nll_loss 2.368 | ppl 5.16 | wps 4695.4 | wpb 143.6 | bsz 17 | num_updates 1520 | best_loss 6.172
2024-01-07 12:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1520 updates
2024-01-07 12:25:45 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1813.pt
2024-01-07 12:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1813.pt
2024-01-07 12:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1813.pt (epoch 38 @ 1520 updates, score 6.181) (writing took 56.29646793502616 seconds)
2024-01-07 12:26:41 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-01-07 12:26:41 | INFO | train | epoch 038 | loss 5.116 | nll_loss 0.874 | ppl 1.83 | wps 112.1 | ups 0.64 | wpb 176.4 | bsz 20.6 | num_updates 1520 | lr 9.6e-06 | gnorm 5.343 | train_wall 6 | gb_free 12.7 | wall 2325
2024-01-07 12:26:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:26:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:26:41 | INFO | fairseq.trainer | begin training epoch 39
2024-01-07 12:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:26:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:26:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:26:47 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.189 | nll_loss 2.424 | ppl 5.37 | wps 4442.6 | wpb 143.6 | bsz 17 | num_updates 1560 | best_loss 6.172
2024-01-07 12:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1560 updates
2024-01-07 12:26:47 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1894.pt
2024-01-07 12:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1894.pt
2024-01-07 12:27:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1894.pt (epoch 39 @ 1560 updates, score 6.189) (writing took 59.44604619796155 seconds)
2024-01-07 12:27:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-01-07 12:27:47 | INFO | train | epoch 039 | loss 5.09 | nll_loss 0.849 | ppl 1.8 | wps 106.8 | ups 0.61 | wpb 176.4 | bsz 20.6 | num_updates 1560 | lr 8.8e-06 | gnorm 5.866 | train_wall 6 | gb_free 12.7 | wall 2391
2024-01-07 12:27:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:27:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:27:47 | INFO | fairseq.trainer | begin training epoch 40
2024-01-07 12:27:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:27:53 | INFO | train_inner | epoch 040:     40 / 40 loss=5.089, nll_loss=0.849, ppl=1.8, wps=134.2, ups=0.76, wpb=177.2, bsz=20.6, num_updates=1600, lr=8e-06, gnorm=6.852, train_wall=15, gb_free=12.7, wall=2398
2024-01-07 12:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:27:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:27:53 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.177 | nll_loss 2.398 | ppl 5.27 | wps 4709.1 | wpb 143.6 | bsz 17 | num_updates 1600 | best_loss 6.172
2024-01-07 12:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1600 updates
2024-01-07 12:27:53 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1771.pt
2024-01-07 12:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1771.pt
2024-01-07 12:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1771.pt (epoch 40 @ 1600 updates, score 6.177) (writing took 58.7285003119614 seconds)
2024-01-07 12:28:52 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-01-07 12:28:52 | INFO | train | epoch 040 | loss 5.077 | nll_loss 0.838 | ppl 1.79 | wps 108.1 | ups 0.61 | wpb 176.4 | bsz 20.6 | num_updates 1600 | lr 8e-06 | gnorm 8.528 | train_wall 6 | gb_free 12.7 | wall 2457
2024-01-07 12:28:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:28:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:28:52 | INFO | fairseq.trainer | begin training epoch 41
2024-01-07 12:28:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:28:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:28:59 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.157 | nll_loss 2.339 | ppl 5.06 | wps 4593.3 | wpb 143.6 | bsz 17 | num_updates 1640 | best_loss 6.157
2024-01-07 12:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1640 updates
2024-01-07 12:28:59 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:29:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:30:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 41 @ 1640 updates, score 6.157) (writing took 114.87276778800879 seconds)
2024-01-07 12:30:54 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-01-07 12:30:54 | INFO | train | epoch 041 | loss 5.067 | nll_loss 0.832 | ppl 1.78 | wps 58.1 | ups 0.33 | wpb 176.4 | bsz 20.6 | num_updates 1640 | lr 7.2e-06 | gnorm 4.57 | train_wall 6 | gb_free 12.7 | wall 2578
2024-01-07 12:30:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:30:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:30:54 | INFO | fairseq.trainer | begin training epoch 42
2024-01-07 12:30:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:31:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:31:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:31:00 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.173 | nll_loss 2.366 | ppl 5.15 | wps 4684.2 | wpb 143.6 | bsz 17 | num_updates 1680 | best_loss 6.157
2024-01-07 12:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1680 updates
2024-01-07 12:31:00 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1731.pt
2024-01-07 12:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1731.pt
2024-01-07 12:31:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1731.pt (epoch 42 @ 1680 updates, score 6.173) (writing took 57.50376127602067 seconds)
2024-01-07 12:31:58 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-01-07 12:31:58 | INFO | train | epoch 042 | loss 5.035 | nll_loss 0.807 | ppl 1.75 | wps 110 | ups 0.62 | wpb 176.4 | bsz 20.6 | num_updates 1680 | lr 6.4e-06 | gnorm 4.676 | train_wall 6 | gb_free 12.7 | wall 2642
2024-01-07 12:31:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:31:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:31:58 | INFO | fairseq.trainer | begin training epoch 43
2024-01-07 12:31:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:32:01 | INFO | train_inner | epoch 043:     20 / 40 loss=5.047, nll_loss=0.813, ppl=1.76, wps=71.5, ups=0.4, wpb=177.3, bsz=20.7, num_updates=1700, lr=6e-06, gnorm=4.515, train_wall=15, gb_free=12.7, wall=2646
2024-01-07 12:32:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:32:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:32:05 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.171 | nll_loss 2.365 | ppl 5.15 | wps 4538.1 | wpb 143.6 | bsz 17 | num_updates 1720 | best_loss 6.157
2024-01-07 12:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1720 updates
2024-01-07 12:32:05 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1714.pt
2024-01-07 12:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1714.pt
2024-01-07 12:33:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1714.pt (epoch 43 @ 1720 updates, score 6.171) (writing took 59.354087681975216 seconds)
2024-01-07 12:33:04 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-01-07 12:33:04 | INFO | train | epoch 043 | loss 5.032 | nll_loss 0.795 | ppl 1.73 | wps 106.8 | ups 0.61 | wpb 176.4 | bsz 20.6 | num_updates 1720 | lr 5.6e-06 | gnorm 4.639 | train_wall 6 | gb_free 12.7 | wall 2708
2024-01-07 12:33:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:33:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:33:04 | INFO | fairseq.trainer | begin training epoch 44
2024-01-07 12:33:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:33:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:33:11 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.196 | nll_loss 2.432 | ppl 5.4 | wps 4621.5 | wpb 143.6 | bsz 17 | num_updates 1760 | best_loss 6.157
2024-01-07 12:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1760 updates
2024-01-07 12:33:11 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:33:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 44 @ 1760 updates, score 6.196) (writing took 27.96190277300775 seconds)
2024-01-07 12:33:39 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-01-07 12:33:39 | INFO | train | epoch 044 | loss 5.031 | nll_loss 0.802 | ppl 1.74 | wps 204.1 | ups 1.16 | wpb 176.4 | bsz 20.6 | num_updates 1760 | lr 4.8e-06 | gnorm 4.953 | train_wall 6 | gb_free 12.7 | wall 2743
2024-01-07 12:33:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:33:39 | INFO | fairseq.trainer | begin training epoch 45
2024-01-07 12:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:33:45 | INFO | train_inner | epoch 045:     40 / 40 loss=5.024, nll_loss=0.794, ppl=1.73, wps=169.3, ups=0.97, wpb=175.4, bsz=20.5, num_updates=1800, lr=4e-06, gnorm=4.879, train_wall=15, gb_free=12.7, wall=2749
2024-01-07 12:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:33:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:33:45 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.176 | nll_loss 2.398 | ppl 5.27 | wps 4699.3 | wpb 143.6 | bsz 17 | num_updates 1800 | best_loss 6.157
2024-01-07 12:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1800 updates
2024-01-07 12:33:45 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1764.pt
2024-01-07 12:34:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1764.pt
2024-01-07 12:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1764.pt (epoch 45 @ 1800 updates, score 6.176) (writing took 55.97350679902593 seconds)
2024-01-07 12:34:41 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-01-07 12:34:41 | INFO | train | epoch 045 | loss 5.011 | nll_loss 0.782 | ppl 1.72 | wps 112.7 | ups 0.64 | wpb 176.4 | bsz 20.6 | num_updates 1800 | lr 4e-06 | gnorm 4.647 | train_wall 6 | gb_free 12.7 | wall 2806
2024-01-07 12:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:34:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:34:41 | INFO | fairseq.trainer | begin training epoch 46
2024-01-07 12:34:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:34:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:34:48 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.177 | nll_loss 2.407 | ppl 5.3 | wps 4648.9 | wpb 143.6 | bsz 17 | num_updates 1840 | best_loss 6.157
2024-01-07 12:34:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1840 updates
2024-01-07 12:34:48 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt
2024-01-07 12:35:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_last.pt (epoch 46 @ 1840 updates, score 6.177) (writing took 31.112478155002464 seconds)
2024-01-07 12:35:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-01-07 12:35:19 | INFO | train | epoch 046 | loss 5.019 | nll_loss 0.794 | ppl 1.73 | wps 188 | ups 1.07 | wpb 176.4 | bsz 20.6 | num_updates 1840 | lr 3.2e-06 | gnorm 5.069 | train_wall 6 | gb_free 12.7 | wall 2843
2024-01-07 12:35:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:35:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:35:19 | INFO | fairseq.trainer | begin training epoch 47
2024-01-07 12:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:35:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:35:25 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.154 | nll_loss 2.349 | ppl 5.09 | wps 4495.4 | wpb 143.6 | bsz 17 | num_updates 1880 | best_loss 6.154
2024-01-07 12:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1880 updates
2024-01-07 12:35:25 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt
2024-01-07 12:37:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint_best.pt (epoch 47 @ 1880 updates, score 6.154) (writing took 103.57773155497853 seconds)
2024-01-07 12:37:09 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-01-07 12:37:09 | INFO | train | epoch 047 | loss 5.015 | nll_loss 0.789 | ppl 1.73 | wps 64 | ups 0.36 | wpb 176.4 | bsz 20.6 | num_updates 1880 | lr 2.4e-06 | gnorm 5.699 | train_wall 6 | gb_free 12.7 | wall 2953
2024-01-07 12:37:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:37:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:37:09 | INFO | fairseq.trainer | begin training epoch 48
2024-01-07 12:37:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:37:12 | INFO | train_inner | epoch 048:     20 / 40 loss=5.009, nll_loss=0.783, ppl=1.72, wps=85.2, ups=0.48, wpb=176.7, bsz=20.6, num_updates=1900, lr=2e-06, gnorm=5.453, train_wall=15, gb_free=12.7, wall=2956
2024-01-07 12:37:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:37:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:37:15 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.173 | nll_loss 2.401 | ppl 5.28 | wps 4717.1 | wpb 143.6 | bsz 17 | num_updates 1920 | best_loss 6.154
2024-01-07 12:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1920 updates
2024-01-07 12:37:15 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1730.pt
2024-01-07 12:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1730.pt
2024-01-07 12:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1730.pt (epoch 48 @ 1920 updates, score 6.173) (writing took 53.79439003003063 seconds)
2024-01-07 12:38:09 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-01-07 12:38:09 | INFO | train | epoch 048 | loss 5.005 | nll_loss 0.779 | ppl 1.72 | wps 116.9 | ups 0.66 | wpb 176.4 | bsz 20.6 | num_updates 1920 | lr 1.6e-06 | gnorm 5.116 | train_wall 6 | gb_free 12.7 | wall 3014
2024-01-07 12:38:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:38:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:38:09 | INFO | fairseq.trainer | begin training epoch 49
2024-01-07 12:38:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:38:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:38:16 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.169 | nll_loss 2.407 | ppl 5.3 | wps 4714.9 | wpb 143.6 | bsz 17 | num_updates 1960 | best_loss 6.154
2024-01-07 12:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1960 updates
2024-01-07 12:38:16 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1692.pt
2024-01-07 12:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1692.pt
2024-01-07 12:39:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1692.pt (epoch 49 @ 1960 updates, score 6.169) (writing took 51.533323136973195 seconds)
2024-01-07 12:39:07 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-01-07 12:39:07 | INFO | train | epoch 049 | loss 4.998 | nll_loss 0.773 | ppl 1.71 | wps 121.4 | ups 0.69 | wpb 176.4 | bsz 20.6 | num_updates 1960 | lr 8e-07 | gnorm 4.676 | train_wall 6 | gb_free 12.7 | wall 3072
2024-01-07 12:39:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:39:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 40
2024-01-07 12:39:07 | INFO | fairseq.trainer | begin training epoch 50
2024-01-07 12:39:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-01-07 12:39:14 | INFO | train_inner | epoch 050:     40 / 40 loss=5.007, nll_loss=0.783, ppl=1.72, wps=144.7, ups=0.82, wpb=176.1, bsz=20.6, num_updates=2000, lr=0, gnorm=4.529, train_wall=15, gb_free=12.7, wall=3078
2024-01-07 12:39:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-01-07 12:39:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:39:14 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.167 | nll_loss 2.396 | ppl 5.27 | wps 4664.5 | wpb 143.6 | bsz 17 | num_updates 2000 | best_loss 6.154
2024-01-07 12:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2000 updates
2024-01-07 12:39:14 | INFO | fairseq.trainer | Saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1672.pt
2024-01-07 12:39:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1672.pt
2024-01-07 12:40:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home2/chu/fairseq_sign/sign/experiments/mt/de-en/sp-10/checkpoint.best_loss_6.1672.pt (epoch 50 @ 2000 updates, score 6.167) (writing took 52.820414260961115 seconds)
2024-01-07 12:40:07 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-01-07 12:40:07 | INFO | train | epoch 050 | loss 5.006 | nll_loss 0.78 | ppl 1.72 | wps 118.5 | ups 0.67 | wpb 176.4 | bsz 20.6 | num_updates 2000 | lr 0 | gnorm 4.395 | train_wall 6 | gb_free 12.7 | wall 3131
2024-01-07 12:40:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-01-07 12:40:07 | INFO | fairseq_cli.train | done training in 3117.2 seconds
/home2/chu/fairseq_sign/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
